{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "import os\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ValueWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"m_speed_stddev_480\",\n",
    "    \"m_acceleration_stddev_480\",\n",
    "    \"m_jerk_stddev_480\",\n",
    "    \"m_steering_stddev_480\",\n",
    "    \"AccelInput_stddev_480\",\n",
    "    \"BrakeInput_stddev_480\",\n",
    "    \"realtime steering entropy_1100\",\n",
    "    \"realtime steering entropy_1100_stddev_480\",\n",
    "    \"perclos\",\n",
    "]\n",
    "TRAIN_DIR = 'dms_data/train/'\n",
    "TEST_DIR = 'dms_data/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['feature1', 'feature2'], ['feature1', 'feature3'], ['feature1', 'feature4'], ['feature2', 'feature3'], ['feature2', 'feature4'], ['feature3', 'feature4']]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_combinations(features, number): return [list(combination) for combination in itertools.combinations(features, number)]\n",
    "\n",
    "# Example usage:\n",
    "sample_features = [\"feature1\", \"feature2\", \"feature3\", \"feature4\"]\n",
    "number = 2\n",
    "combinations = generate_combinations(sample_features, number)\n",
    "print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solveOne(train_csv_path,test_csv_path,feature_columns,target,file_name=''):\n",
    "  \"\"\"\n",
    "  :param train_csv_path: path to train csv file\n",
    "  :param test_csv_path: path to test csv file\n",
    "  :param feature_columns: list of feature to be used as input\n",
    "  :param target: target dataFrame column\n",
    "  :param file_name: name of the file to save the plot\n",
    "  :return: None\n",
    "  \"\"\"\n",
    "  # train_df = pd.read_csv(path.join(TRAIN_DIR, train_csv))\n",
    "  # test_df = pd.read_csv(path.join(TEST_DIR, test_csv))\n",
    "  train_df = pd.read_csv(train_csv_path)\n",
    "  test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "  train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "  test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "  train_df.set_index('timestamp', inplace=True)\n",
    "  test_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "  # # 頻度追加\n",
    "  # train_df = train_df.resample('20S')\n",
    "  # test_df = test_df.resample('20S')\n",
    "\n",
    "  \n",
    "  feature_scaler = MinMaxScaler()\n",
    "  target_scaler = MinMaxScaler()\n",
    "  train_df[feature_columns] = feature_scaler.fit_transform(train_df[feature_columns])\n",
    "  test_df[feature_columns] = feature_scaler.transform(test_df[feature_columns])\n",
    "  train_df[target] = target_scaler.fit_transform(train_df[[target]])\n",
    "  test_df[target] = target_scaler.transform(test_df[[target]])\n",
    "  \n",
    "  model = ARIMA(train_df[target], order=(5,1,0))\n",
    "  model_fit = model.fit()\n",
    "  predictions = model_fit.forecast(steps=len(test_df))\n",
    "  rmse = np.sqrt(mean_squared_error(test_df[target], predictions))\n",
    "  #print('Test RMSE: %.3f' % rmse)\n",
    "  # plt.title(f'{target} over Time')\n",
    "  # plt.xlabel('Timestamp')\n",
    "  # plt.ylabel(target)\n",
    "  # plt.plot(test_df.index, test_df[target], label='Actual')\n",
    "  # plt.plot(test_df.index, predictions, label='Predicted', color='red')\n",
    "  # plt.legend()\n",
    "  # if file_name != '':\n",
    "  #   plt.savefig(path.join('./figure', file_name))\n",
    "  #   plt.show()\n",
    "  return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from os import path\n",
    "\n",
    "\n",
    "def calculate_average_rmse(train_paths, test_paths, features, target, file_name=''):\n",
    "  rmses = []\n",
    "\n",
    "  for train_csv, test_csv in zip(train_paths, test_paths):\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    test_df = pd.read_csv(test_csv)\n",
    "\n",
    "    train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "    test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "    train_df.set_index('timestamp', inplace=True)\n",
    "    test_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    train_df[features] = feature_scaler.fit_transform(train_df[features])\n",
    "    test_df[features] = feature_scaler.transform(test_df[features])\n",
    "    train_df[target] = target_scaler.fit_transform(train_df[[target]])\n",
    "    test_df[target] = target_scaler.transform(test_df[[target]])\n",
    "\n",
    "    model = ARIMA(train_df[target], order=(5, 1, 0))\n",
    "    model_fit = model.fit()\n",
    "    predictions = model_fit.forecast(steps=len(test_df))\n",
    "    rmse = np.sqrt(mean_squared_error(test_df[target], predictions))\n",
    "    rmses.append(rmse)\n",
    "\n",
    "    # if file_name != '':\n",
    "    #   plt.title(f'{target} over Time')\n",
    "    #   plt.xlabel('Timestamp')\n",
    "    #   plt.ylabel(target)\n",
    "    #   plt.plot(test_df.index, test_df[target], label='Actual')\n",
    "    #   plt.plot(test_df.index, predictions, label='Predicted', color='red')\n",
    "    #   plt.legend()\n",
    "    #   plt.savefig(path.join('./figure', f'{file_name}_{train_csv.split(\"/\")[-1]}'))\n",
    "    #   plt.clf()  # Clear the current figure\n",
    "\n",
    "  average_rmse = np.mean(rmses)\n",
    "  return average_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "table:ARIMA_Sleepiness-0\n",
      " 変数の数\t平均\t1\t2\t3\t4\t5\t6\t7\t8\t9\n",
      " ARIMA\t0.3076501898367987\t0.308\t0.308\t0.308\t0.308\t0.308\t0.308\t0.308\t0.308\t0.308\n",
      "------------------------\n",
      "1\n",
      "table:ARIMA_Sleepiness-1\n",
      " 変数の数\t平均\t1\t2\t3\t4\t5\t6\t7\t8\t9\n",
      " ARIMA\t0.0\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\t0.000\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\" \n",
    "target => 予測対象のカラム\n",
    "\"\"\"\n",
    "target = 'oss'\n",
    "target = 'Sleepiness'\n",
    "\n",
    "csvs = os.listdir(TRAIN_DIR)\n",
    "for i,csv in enumerate(csvs):\n",
    "  if csv.endswith('y_train.csv'):\n",
    "    #人ごとセクション\n",
    "    train_path = path.join(TRAIN_DIR, csv)\n",
    "    test_path = path.join(TEST_DIR, csv.replace('train', 'test'))\n",
    "    list_rmse_with_feature_counts = []\n",
    "    for ii in range(len(features)):\n",
    "      #人+特徴量組み合わせ個数セクション\n",
    "      feature_combinations = generate_combinations(features, ii + 1)\n",
    "      results = []\n",
    "      for feature_combination in feature_combinations:\n",
    "        #人+特徴量組み合わせ個数+特徴量セクション\n",
    "        rmse = solveOne(train_path,test_path,feature_combination,target,'')\n",
    "        results.append(rmse)\n",
    "      average = np.mean(results)\n",
    "      list_rmse_with_feature_counts.append(average)\n",
    "    \n",
    "    column_names = ['変数の数','平均'] + list(map(lambda x: str(x), range(1, len(features) + 1)))\n",
    "    row = ['ARIMA',str(np.mean(list_rmse_with_feature_counts))] + list(map(lambda rmse: '{:.3f}'.format(rmse), list_rmse_with_feature_counts))\n",
    "    print(str(i))\n",
    "    print(f\"table:ARIMA_{target}-{str(i)}\")\n",
    "    print(' ' + '\\t'.join(column_names))\n",
    "    print(' ' + '\\t'.join(row))\n",
    "    print('------------------------')\n",
    "\n",
    "# train_path_list = [path.join(TRAIN_DIR, csv) for csv in csvs if csv.endswith('y_train.csv')]\n",
    "# test_path_list = [path.join(TEST_DIR, csv.replace('train', 'test')) for csv in csvs if csv.endswith('y_train.csv')]\n",
    "\n",
    "# rmse = calculate_average_rmse(train_path_list,test_path_list, features, target)\n",
    "# print(rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
