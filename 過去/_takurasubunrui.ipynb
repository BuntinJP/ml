{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6074074074074074\n",
      "Accuracy: 0.6074074074074074\n",
      "Precision: 0.411715586522756\n",
      "Recall: 0.38492287614263593\n",
      "F1-Score: 0.3840879307281907\n",
      "Confusion Matrix:\n",
      " [[  0   2   3   0   0]\n",
      " [  0  88  67   6   0]\n",
      " [  0  24 196  33   4]\n",
      " [  0   0  31  40   7]\n",
      " [  0   0   8  27   4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buntin/jupyterlab/lib64/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# oss\n",
    "\n",
    "def round_to_nearest_integer(series):\n",
    "  return series.round().astype(int)\n",
    "\n",
    "data_dir = './dms_data/'\n",
    "train_data_path = data_dir + '/train'\n",
    "test_data_path = data_dir + '/test'\n",
    "\n",
    "train_files = os.listdir(train_data_path)\n",
    "train_df_list = []\n",
    "for file in train_files:\n",
    "  if file.endswith('.csv'):\n",
    "    df = pd.read_csv(os.path.join(train_data_path, file))\n",
    "    train_df_list.append(df)\n",
    "train_df = pd.concat(train_df_list, ignore_index=True)\n",
    "\n",
    "test_files = os.listdir(test_data_path)\n",
    "test_df_list = []\n",
    "for file in test_files:\n",
    "  if file.endswith('.csv'):\n",
    "    df = pd.read_csv(os.path.join(test_data_path, file))\n",
    "    test_df_list.append(df)\n",
    "test_df = pd.concat(test_df_list, ignore_index=True)\n",
    "\n",
    "\n",
    "X_train = train_df.drop(columns=[\"timestamp\", \"oss\", \"Sleepiness\"], errors=\"ignore\")\n",
    "y_train = round_to_nearest_integer(train_df['oss'])\n",
    "\n",
    "X_test = test_df.drop(columns=[\"timestamp\", \"oss\", \"Sleepiness\"], errors=\"ignore\")\n",
    "y_test = round_to_nearest_integer(test_df['oss'])\n",
    "\n",
    "\n",
    "X_train = X_train.dropna()\n",
    "y_train = y_train[X_train.index]\n",
    "\n",
    "X_test = X_test.dropna()\n",
    "y_test = y_test[X_test.index]\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Precision, Recall, F1-Scoreを計算\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Confusion Matrixを計算\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buntin/jupyterlab/lib64/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories [1] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m encoder \u001b[39m=\u001b[39m OneHotEncoder(sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m y_train_encoded \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mfit_transform(y_train\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m y_test_encoded \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mtransform(y_test\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[1;32m      6\u001b[0m \u001b[39m# Neural Network Model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n",
      "File \u001b[0;32m~/jupyterlab/lib64/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/jupyterlab/lib64/python3.11/site-packages/sklearn/preprocessing/_encoders.py:917\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[1;32m    913\u001b[0m warn_on_unknown \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_unknown \u001b[39min\u001b[39;00m {\n\u001b[1;32m    914\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    915\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minfrequent_if_exist\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    916\u001b[0m }\n\u001b[0;32m--> 917\u001b[0m X_int, X_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(\n\u001b[1;32m    918\u001b[0m     X,\n\u001b[1;32m    919\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[1;32m    920\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    921\u001b[0m     warn_on_unknown\u001b[39m=\u001b[39;49mwarn_on_unknown,\n\u001b[1;32m    922\u001b[0m )\n\u001b[1;32m    923\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_infrequent_categories(X_int, X_mask)\n\u001b[1;32m    925\u001b[0m n_samples, n_features \u001b[39m=\u001b[39m X_int\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/jupyterlab/lib64/python3.11/site-packages/sklearn/preprocessing/_encoders.py:174\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m handle_unknown \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    170\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    171\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound unknown categories \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m in column \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m during transform\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(diff, i)\n\u001b[1;32m    173\u001b[0m     )\n\u001b[0;32m--> 174\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[39mif\u001b[39;00m warn_on_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: Found unknown categories [1] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding for Neural Network\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_encoded = encoder.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Neural Network Model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(y_train_encoded.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_encoded, epochs=10, batch_size=32)\n",
    "\n",
    "# Neural Network Prediction\n",
    "y_pred_nn_prob = model.predict(X_test)\n",
    "y_pred_nn = np.argmax(y_pred_nn_prob, axis=1)\n",
    "\n",
    "# Neural Network Accuracy\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "print(\"Neural Network Accuracy:\", accuracy_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] Label must be in [0, 4), but found 4 in label\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Label must be in [0, 4), but found 4 in label",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m dtrain \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(X_train, label\u001b[39m=\u001b[39my_train_encoded)\n\u001b[1;32m     13\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m     14\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_class\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_train_encoded)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     22\u001b[0m }\n\u001b[0;32m---> 23\u001b[0m clf_lgb \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(params, dtrain, \u001b[39m100\u001b[39;49m)\n\u001b[1;32m     24\u001b[0m y_pred_lgb_encoded \u001b[39m=\u001b[39m clf_lgb\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     25\u001b[0m y_pred_lgb_encoded \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_pred_lgb_encoded, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/jupyterlab/lib64/python3.11/site-packages/lightgbm/engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39m# construct booster\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     booster \u001b[39m=\u001b[39m Booster(params\u001b[39m=\u001b[39;49mparams, train_set\u001b[39m=\u001b[39;49mtrain_set)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    273\u001b[0m         booster\u001b[39m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m~/jupyterlab/lib64/python3.11/site-packages/lightgbm/basic.py:2610\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2608\u001b[0m params_str \u001b[39m=\u001b[39m param_dict_to_str(params)\n\u001b[1;32m   2609\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_void_p()\n\u001b[0;32m-> 2610\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterCreate(\n\u001b[1;32m   2611\u001b[0m     train_set\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   2612\u001b[0m     c_str(params_str),\n\u001b[1;32m   2613\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle)))\n\u001b[1;32m   2614\u001b[0m \u001b[39m# save reference to data\u001b[39;00m\n\u001b[1;32m   2615\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_set \u001b[39m=\u001b[39m train_set\n",
      "File \u001b[0;32m~/jupyterlab/lib64/python3.11/site-packages/lightgbm/basic.py:125\u001b[0m, in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m    The return value from C API calls.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(_LIB\u001b[39m.\u001b[39mLGBM_GetLastError()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mLightGBMError\u001b[0m: Label must be in [0, 4), but found 4 in label"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "all_y = pd.concat([y_train, y_test])\n",
    "all_y_encoded = label_encoder.fit_transform(all_y)\n",
    "\n",
    "y_train_encoded = all_y_encoded[:len(y_train)]\n",
    "y_test_encoded = all_y_encoded[len(y_train):]\n",
    "\n",
    "# LightGBMでの学習と予測\n",
    "dtrain = lgb.Dataset(X_train, label=y_train_encoded)\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(np.unique(y_train_encoded)),\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'multi_logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "clf_lgb = lgb.train(params, dtrain, 100)\n",
    "y_pred_lgb_encoded = clf_lgb.predict(X_test)\n",
    "y_pred_lgb_encoded = np.argmax(y_pred_lgb_encoded, axis=1)\n",
    "\n",
    "y_pred_lgb = label_encoder.inverse_transform(y_pred_lgb_encoded)\n",
    "\n",
    "# LightGBMの精度計算\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "print(\"LightGBM Accuracy:\", accuracy_lgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
