{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 不使用\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 未インストール\n",
    "# import optuna\n",
    "# from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [3, 6, 9, 12, 15, 18, 19, 21, 22, 24, 25]\n",
    "\n",
    "train_paths = [\n",
    "    \"20201127_1548_2_y_train.csv\",\n",
    "    \"20201210_1112_2_y_train.csv\",\n",
    "    \"20201210_1354_2_y_train.csv\",\n",
    "    \"20201127_1840_5_y_train.csv\",\n",
    "    \"20201130_1122_5_y_train.csv\",\n",
    "    \"20201201_1429_5_y_train.csv\",\n",
    "    \"20201203_1244_5_y_train.csv\",\n",
    "    \"20201130_1808_6_y_train.csv\",\n",
    "    \"20201203_1404_6_y_train.csv\",\n",
    "    \"20201210_1610_6_y_train.csv\",\n",
    "    \"20201127_1432_7_y_train.csv\",\n",
    "    \"20201127_1701_7_y_train.csv\",\n",
    "    \"20201203_1022_7_y_train.csv\"\n",
    "]\n",
    "test_paths = [\n",
    "    \"20201126_1546_0_y_train.csv\",\n",
    "    \"20201201_1230_0_y_train.csv\",\n",
    "    \"20201201_1555_0_y_train.csv\"\n",
    "]\n",
    "\n",
    "DIR = './dms_data/train/'\n",
    "\n",
    "train_paths = map(lambda x: os.path.join(DIR, x), train_paths)\n",
    "test_paths = map(lambda x: os.path.join(DIR, x), test_paths)\n",
    "\n",
    "# デバイスの設定\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "# バッチサイズの指定（2^nで指定するのが慣例）\n",
    "BATCHSIZE = 128\n",
    "# EPOCHS数の指定。何回データ全体の学習を行うか（10を指定すると丸々データを10回訓練する）\n",
    "EPOCHS = 10\n",
    "# 1epochあたりの最大訓練＆テストデータサンプル数を指定（バッチサイズ*30とすると30個にデータが分割）\n",
    "# 読み込んだデータがこの値より低いと関係ない\n",
    "N_TRAIN_EXAMPLES = BATCHSIZE * 50\n",
    "N_VALID_EXAMPLES = BATCHSIZE * 20\n",
    "# 正規化数値\n",
    "MIN_VAL = 1.0\n",
    "MAX_VAL = 5.0\n",
    "\n",
    "features_name = [\n",
    "    \"m_speed_stddev_480\",\n",
    "    \"m_acceleration_stddev_480\",\n",
    "    \"m_jerk_stddev_480\",\n",
    "    \"m_steering_stddev_480\",\n",
    "    \"AccelInput_stddev_480\",\n",
    "    \"BrakeInput_stddev_480\",\n",
    "    \"realtime steering entropy_1100\",\n",
    "    \"realtime steering entropy_1100_stddev_480\",\n",
    "    \"perclos\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "    super(LSTMModel, self).__init__()\n",
    "    self.lstm = nn.LSTM(input_size, hidden_size,\n",
    "                        num_layers, batch_first=True)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "  def forward(self, x):\n",
    "    out, _ = self.lstm(x)\n",
    "    out = self.fc(out[:, -1, :])  # 最後の時間ステップの出力を取得\n",
    "    return out\n",
    "\n",
    "def normalize_target_variable(y_train, y_test, min_val, max_val):\n",
    "  y_train_s = (y_train - min_val) / (max_val - min_val)\n",
    "  y_test_s = (y_test - min_val) / (max_val - min_val)\n",
    "  return y_train_s, y_test_s\n",
    "\n",
    "# モデルの定義\n",
    "def define_model(features_combi):\n",
    "  # 層の数、各層のユニット数、およびドロップアウトの割合をOptunaから受け取り、MLPのモデルを構築\n",
    "  # n_layersで層の数を指定\n",
    "  n_layers = 3\n",
    "  layers = []\n",
    "  # 入力層の入力サイズを指定する。最初は特徴量の数に合わせる\n",
    "  in_features = features_combi\n",
    "  # 定義した層数分を回して、ユニット数＆\n",
    "  for i in range(n_layers):\n",
    "    # 出力数を指定する（次の層のユニット数になる（in_featuresになる））\n",
    "    out_features = in_features + 2\n",
    "    # nn.Linearは線形変換を行うためのモジュールで、全結合層を生成。入力と重み行列の行列積を計算し、バイアスを加えることで線形変換を行う。\n",
    "    # バイアスをTrueにすることでバイアスを追加する（default=True)\n",
    "    layers.append(nn.Linear(in_features, out_features, bias=True))\n",
    "    # 活性化関数を指定。非線形に変換する。\n",
    "    # 使える活性化関数はドキュメントへ→『Non-linear Activations (weighted sum, nonlinearity)』\n",
    "    # https://pytorch.org/docs/stable/nn.html#non-linear-activations-other\n",
    "    layers.append(nn.ReLU())\n",
    "    # ドロップアウトは、訓練時にランダムに一部のノードを無効にすることで、過学習を防ぐ\n",
    "    # ドロップアウトの確率を指定する(0.5だったら50%)\n",
    "    p = 0.5\n",
    "    layers.append(nn.Dropout(p))\n",
    "    # 次の入力は前の出力層になるため数を同じに\n",
    "    in_features = out_features\n",
    "  # print(layers)\n",
    "  # 最後に出力層として出力サイズを指定（回帰の場合は1に）\n",
    "  layers.append(nn.Linear(in_features, 1))\n",
    "  # print(layers)\n",
    "  # print(\"-----\")\n",
    "  # 回帰問題ではLogSoftmaxは不要なので削除\n",
    "  # layers.append(nn.LogSoftmax(dim=1))\n",
    "  return nn.Sequential(*layers)\n",
    "\n",
    "def load_local_data(csv_train_path, csv_test_path):\n",
    "  # 選択する特徴量を決める\n",
    "  # #目的変数を決める\n",
    "  objective_variable = \"oss\"\n",
    "\n",
    "  # 引数のcsv_train_path, csv_test_pathからデータをtrai_dataとtrain_combined_dataとtest_combined_dataに追加していく\n",
    "  train_combined_data = []\n",
    "  test_combined_data = []\n",
    "  for train_path, test_path in zip(csv_train_path, csv_test_path):\n",
    "    train_data = pd.read_csv(train_path, usecols=cols)\n",
    "    test_data = pd.read_csv(test_path, usecols=cols)\n",
    "    train_combined_data.append(train_data)\n",
    "    test_combined_data.append(test_data)\n",
    "\n",
    "  # リストに追加されていったデータを1つに繋げる\n",
    "  df_train = pd.concat(train_combined_data, axis=0, ignore_index=True)\n",
    "  df_test = pd.concat(test_combined_data, axis=0, ignore_index=True)\n",
    "\n",
    "  # nanなどのデータを削除\n",
    "  df_train = df_train.dropna()\n",
    "  df_test = df_test.dropna()\n",
    "  df_train = df_train.reset_index(drop=True)\n",
    "  df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "  # 特徴量とラベルの分割\n",
    "  # 訓練用の特徴量とラベル\n",
    "  X_train = df_train[features].values\n",
    "  y_train = df_train[objective_variable].values\n",
    "  # テスト用の特徴量とラベル\n",
    "  X_valid = df_test[features].values\n",
    "  y_valid = df_test[objective_variable].values\n",
    "\n",
    "  # ラベルの正規化\n",
    "  y_train, y_valid = normalize_target_variable(\n",
    "      y_train, y_valid, MIN_VAL, MAX_VAL\n",
    "  )\n",
    "\n",
    "  # PyTorchのテンソルに変換\n",
    "  # PyTorchでのデータの扱いに適した形式にデータを変換 dtypeでデータ型を選択\n",
    "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "  y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "  X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "  y_valid_tensor = torch.tensor(y_valid, dtype=torch.float32)\n",
    "  # DataLoaderの作成\n",
    "  # 同じインデックス位置のデータをラップ（まとめ）する\n",
    "  '''tensor1 = torch.tensor([1, 2, 3])\n",
    "       tensor2 = torch.tensor([4, 5, 6])\n",
    "       dataset = TensorDataset(tensor1, tensor2)\n",
    "       (tensor(1), tensor(4))\n",
    "       (tensor(2), tensor(5))\n",
    "       (tensor(3), tensor(6))'''\n",
    "  train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "  valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "  '''DataLoaderを行うことで指定したバッチサイズにdata(特徴量)target(ラベル)に分けてくれる。\n",
    "        (tensor(1), tensor(4))\n",
    "        (tensor(2), tensor(5))\n",
    "        (tensor(3), tensor(6))\n",
    "        batch_size = 2\n",
    "        Batch Data: tensor([1, 2])\n",
    "        Batch Target: tensor([4, 5])\n",
    "        Batch Data: tensor([3])\n",
    "        Batch Target: tensor([6])\n",
    "        for batch in data_loader:\n",
    "          data, target = batch\n",
    "        print(\"Batch Data:\", data)\n",
    "        print(\"Batch Target:\", target)'''\n",
    "  train_loader = DataLoader(\n",
    "      train_dataset, batch_size=BATCHSIZE, shuffle=False)\n",
    "  valid_loader = DataLoader(\n",
    "      valid_dataset, batch_size=BATCHSIZE, shuffle=False)\n",
    "\n",
    "  return train_loader, valid_loader\n",
    "\n",
    "def normalize_target_variable(y_train, y_test, min_val, max_val):\n",
    "  y_train_s = (y_train - min_val) / (max_val - min_val)\n",
    "  y_test_s = (y_test - min_val) / (max_val - min_val)\n",
    "  return y_train_s, y_test_s\n",
    "\n",
    "def calculate_rmse(predictions, targets):\n",
    "  return torch.sqrt(nn.functional.mse_loss(predictions, targets))\n",
    "\n",
    "def combi(n):\n",
    "  global features_name\n",
    "  comb_list = list(itertools.combinations(features_name, n))\n",
    "  return comb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "9\n",
      "36\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 2, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# modelに特徴量を入れて学習させ予測値を出す\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# output(予測値)とラベルの誤差をRMSEで求める。（squeeze()はテンソルからサイズが1の次元を取り除く。outputの最後にはgrad_fn=<AddmmBackward0>のPytorchの自動微分が入ってるため。\u001b[39;00m\n\u001b[1;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m calculate_rmse(output\u001b[38;5;241m.\u001b[39msqueeze(), target\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/jupyter-env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 8\u001b[0m   out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# 最後の時間ステップの出力を取得\u001b[39;00m\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/jupyter-env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/jupyter-env/lib64/python3.9/site-packages/torch/nn/modules/rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m~/jupyter-env/lib64/python3.9/site-packages/torch/nn/modules/rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    726\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    729\u001b[0m                        ):\n\u001b[0;32m--> 730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    732\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    734\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/jupyter-env/lib64/python3.9/site-packages/torch/nn/modules/rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    216\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    220\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 2, got 1"
     ]
    }
   ],
   "source": [
    "train_combined_data = []\n",
    "test_combined_data = []\n",
    "for train_path in train_paths:\n",
    "  train_data = pd.read_csv(train_path, usecols=cols)\n",
    "  train_combined_data.append(train_data)\n",
    "\n",
    "for test_path in test_paths:\n",
    "  test_data = pd.read_csv(test_path, usecols=cols)\n",
    "  test_combined_data.append(test_data)\n",
    "train = pd.concat(train_combined_data, axis=0, ignore_index=True)\n",
    "test = pd.concat(test_combined_data, axis=0, ignore_index=True)\n",
    "\n",
    "# Pandasのデータフレームを作成\n",
    "df_train = pd.DataFrame(train)\n",
    "df_test = pd.DataFrame(test)\n",
    "\n",
    "# 欠損値の削除とインデックスの再設定\n",
    "df_train = df_train.dropna()\n",
    "df_test = df_test.dropna()\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "result_list = []\n",
    "\n",
    "start = time.perf_counter()\n",
    "for iteration in range(1):\n",
    "  print(\"iteration:\", iteration)\n",
    "  count = 0\n",
    "  cnt = 0\n",
    "  min_val = 1.0\n",
    "  max_val = 5.0\n",
    "  rmse_min = []\n",
    "  feature_min = []\n",
    "  for n in range(1, 10):\n",
    "    x_col = combi(n)\n",
    "    min_rmse = float(\"inf\")\n",
    "    f = -1\n",
    "    print(len(x_col))\n",
    "    for i in range(len(x_col)):\n",
    "      features = x_col[i]\n",
    "      X_train = df_train[list(features)].values\n",
    "      y_train = df_train[\"oss\"].values\n",
    "      # テストデータの特徴量を選択\n",
    "      X_test = df_test[list(features)].values\n",
    "      y_test = df_test[\"oss\"].values\n",
    "      X_train = X_train.reshape(\n",
    "          X_train.shape[0], X_train.shape[1], 1)\n",
    "      X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "      # 正規化\n",
    "      y_train_s, y_test_s = normalize_target_variable(\n",
    "          y_train, y_test, min_val, max_val\n",
    "      )\n",
    "      X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "      y_train_tensor = torch.tensor(y_train_s, dtype=torch.float32)\n",
    "      X_valid_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "      y_valid_tensor = torch.tensor(y_test_s, dtype=torch.float32)\n",
    "      # DataLoaderの作成\n",
    "      # 同じインデックス位置のデータをラップ（まとめ）する\n",
    "      train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "      valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "      train_loader = DataLoader(\n",
    "          train_dataset, batch_size=BATCHSIZE, shuffle=False)\n",
    "      valid_loader = DataLoader(\n",
    "          valid_dataset, batch_size=BATCHSIZE, shuffle=False)\n",
    "      # モデルの生成\n",
    "      # Optunaのトライアル（ハイパーパラメータの探索）から受け取ったハイパーパラメータを使用して、モデルのアーキテクチャを構築。nn.Module を返す\n",
    "      model = LSTMModel(input_size=n, hidden_size=64,\n",
    "                        num_layers=3, output_size=1).to(DEVICE)\n",
    "      # オプティマイザの生成F\n",
    "      optimizer_name = \"Adam\"  # 最適化アルゴリズムの選択\n",
    "      lr = 0.001  # 学習率の選択\n",
    "      optimizer = getattr(optim, optimizer_name)(model.parameters(\n",
    "      ), lr=lr, weight_decay=0.0001)  # PyTorchの最適化アルゴリズムの作成 weight_decay=0.0001は正則化L2項\n",
    "      # モデルのトレーニング\n",
    "      # EPOCHS数だけ回す（データ件数が10でBachSizeが2なら、5回繰り返すと、10件のデータを処理。 この1サイクルのことをEpochと呼ぶ。\n",
    "      for epoch in range(EPOCHS):\n",
    "        # model.train()で訓練モードに。訓練モードにすることでdropoutを有効にする\n",
    "        # 訓練時にランダムに一部のユニットを無効にすることで、異なる部分ネットワークを学習させ、モデルが特定のパターンに依存しすぎないようにする。\n",
    "        model.train()\n",
    "        # batch_idxにはtrain_loaderの中身に入っているバッチ数が、dataには特徴量、testにはラベルが入る（データセット取得の時にload_local_dataでshuffleがFalseなら順番に）\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "          # トレーニングデータを制限（現在の繰り返しでデータ数より上回るを処理を行うとループを抜ける（ほぼ使わない））\n",
    "          if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n",
    "            break\n",
    "          # .to()はPyTorchのテンソルやモデルを指定したデバイスに移動するためのメソッド。CPUかGPU(cuda)を指定できる\n",
    "          data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "          # 逆伝播で各パラメータの勾配（偏微分）を計算し、それを使用してモデルのパラメータを更新するが、勾配が累積するためミニバッチごとに勾配を0にする。\n",
    "          optimizer.zero_grad()\n",
    "          # modelに特徴量を入れて学習させ予測値を出す\n",
    "          output = model(data)\n",
    "          # output(予測値)とラベルの誤差をRMSEで求める。（squeeze()はテンソルからサイズが1の次元を取り除く。outputの最後にはgrad_fn=<AddmmBackward0>のPytorchの自動微分が入ってるため。\n",
    "          loss = calculate_rmse(output.squeeze(), target.float())\n",
    "          # loss(損失)に対する各パラメータの勾配（偏微分）を計算する。（逆伝播）\n",
    "          loss.backward()\n",
    "          # 設定したPyTorchの最適化アルゴリズムが計算された勾配を使用して、モデルのパラメータを更新\n",
    "          optimizer.step()\n",
    "        # モデルの検証\n",
    "        # 訓練モードで作ったモデルで検証を行う。そのため検証モードにし、Dropoutを無効にして訓練データを全て使用するようにする。\n",
    "        model.eval()\n",
    "        # 検証モードのため勾配の計算を無効にする。\n",
    "        with torch.no_grad():\n",
    "          # 1EPOCHS分をvalid_lossに格納する。（最後に平均を取るため）\n",
    "          valid_loss = 0.0\n",
    "          # 検証用のデータをdata,targetに分ける。\n",
    "          for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "            # 検証データを制限\n",
    "            if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n",
    "              break\n",
    "            # .to()はPyTorchのテンソルやモデルを指定したデバイスに移動するためのメソッド。CPUかGPU(cuda)を指定できる\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            # 訓練で作成したモデルに検証用データを入れて予測値を出す\n",
    "            output = model(data)\n",
    "            # 予測値と正解ラベルの誤差を1EPOCHS分を格納する変数に追加\n",
    "            valid_loss += calculate_rmse(\n",
    "                output.squeeze(), target.float()).item()\n",
    "        # RMSEの計算\n",
    "        # valid_loaderのlengthでvalid_lossを割る。valid_loaderの長さはミニバッチの個数に当たるため。\n",
    "        rmse = (valid_loss / len(valid_loader))\n",
    "        if rmse < min_rmse:\n",
    "          min_rmse = rmse\n",
    "          f = i  # fは特徴量の組み合わせのインデックスを表してる。\n",
    "    rmse_min.append(min_rmse)  # 各特徴量毎に最小のRMSEを格納\n",
    "    feature_min.append(x_col[f])  # 各特徴量毎に最適な特徴量の組み合わせを格納\n",
    "  print(rmse_min)\n",
    "  result_list.append({\n",
    "      'iteration': iteration,\n",
    "      'feature_min': feature_min,\n",
    "      'rmse_min': rmse_min,\n",
    "      'rmse_mean': np.mean(rmse_min)\n",
    "  })\n",
    "  # 結果をDataFrameに変換してCSVに保存\n",
    "result_df = pd.DataFrame(result_list)\n",
    "result_df['rmse_min'] = result_df['rmse_min'].apply(\n",
    "    lambda x: ', '.join(map(str, x)))\n",
    "result_df[['1', '2', '3', '4', '5', '6', '7', '8', '9']] = pd.DataFrame(\n",
    "    result_df['rmse_min'].str.split(',').tolist(), dtype=float)\n",
    "result_df.drop(['rmse_min'], axis=1, inplace=True)\n",
    "result_df.to_csv('optuna/3/PytorchNN_oss_固定/results.csv', index=False)\n",
    "\n",
    "end = time.perf_counter() - start\n",
    "elapsed_minutes = int(end // 60)\n",
    "elapsed_seconds = int(end % 60)\n",
    "print(\"----------------------\")\n",
    "print(result_df)\n",
    "print(f\"経過時間: {elapsed_minutes}分 {elapsed_seconds}秒\")\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
